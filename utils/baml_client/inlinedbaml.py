###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "bullet_points.baml": "class BulletPoints {\n  points string[] @description(\"Array of clear, concise bullet points that capture the main ideas\")\n  mainIdea string @description(\"The central theme or main idea of the text\")\n  complexity \"basic\" | \"intermediate\" | \"advanced\" @description(\"The level of complexity of the content\")\n}\n\nfunction ExtractBulletPoints(text: string) -> BulletPoints {\n  client GeminiGroqFallback\n  prompt #\"\n    Create a set of clear and concise bullet points from the given text. Focus on the key ideas and important details.\n    Determine the main idea and assess the complexity level of the content.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ text }}\n  \"#\n}\n\ntest SimpleSubtopicTest {\n  functions [ExtractBulletPoints]\n  args {\n    text \"Photosynthesis is the process by which plants convert sunlight into energy. During this process, plants take in carbon dioxide from the air and water from the soil. Using sunlight as energy, they convert these materials into glucose and oxygen. The glucose serves as food for the plant, while oxygen is released into the atmosphere as a byproduct.\"\n  }\n}\n\ntest ComplexSubtopicTest {\n  functions [ExtractBulletPoints]\n  args {\n    text #\"\n      The Industrial Revolution marked a major turning point in human history. This period saw the transition from manual production methods to machine manufacturing. It began in Britain in the late 18th century and spread to other parts of the world. The revolution led to increased production capacity, urbanization, and significant social changes. Workers moved from rural areas to cities, working conditions were often harsh, and new social classes emerged. Technical innovations like the steam engine and power loom transformed manufacturing forever.\n    \"#\n  }\n}",
    "celerbud.baml": "// Define the structure for chat messages\nclass ChatMessage {\n  role \"user\" | \"assistant\"\n  content string\n  timestamp string @description(\"ISO timestamp of when message was sent\")\n}\n\n// Structure for context from various sources\nclass ContextSource {\n  sourceType \"vector_db\" | \"document\" | \"api\"\n  content string\n  relevanceScore float @description(\"How relevant this context is to the query\")\n}\n\n// Structure for the chatbot's response\nclass ChatResponse {\n  answer string @description(\"The main response content\")\n  usedContext ContextSource[] @description(\"Sources used to generate the response\")\n  confidence \"high\" | \"medium\" | \"low\" @description(\"Confidence level in the response\")\n}\nclass CasualResponse {\n  answer string @stream.with_state @description(\"Casual Natural Language response\")\n  queryUsed string @stream.done @description(\"NIL\")\n  rawResults string @stream.done @description(\"NIL\")\n}\nfunction CasualGenerator(question: string) -> CasualResponse {\n  client GeminiGroqFallback\n  prompt #\"\n    Reply to the user in a casual, friendly manner.\n    Use the following context to provide a relevant answer.\n    If the context doesn't contain relevant information, acknowledge that and provide a general response.\n    App details:\n\n    User: {{ question }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n// Main chat function with streaming\nfunction StreamingChat(\n  messages: ChatMessage[],\n  context: ContextSource[]\n) -> ChatResponse {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a helpful AI assistant. Use the provided context to answer questions accurately.\n    If the context doesn't contain relevant information, acknowledge that and provide a general response.\n\n    Available Context:\n    {% for ctx in context %}\n    Source ({{ ctx.sourceType }}): {{ ctx.content }}\n    {% endfor %}\n\n    Chat History:\n    {% for msg in messages %}\n    {{ _.role(msg.role) }} {{ msg.content }}\n    {% endfor %}\n\n    {{ ctx.output_format }}\n  \"#\n}\n// Non-streaming chat function\nfunction GenerateResponse(\n  messages: ChatMessage[],\n  context: ContextSource[]\n) -> ChatResponse {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a helpful AI assistant. Use the provided context to answer questions accurately.\n    If the context doesn't contain relevant information, acknowledge that and provide a general response.\n\n    Available Context:\n    {% for ctx in context %}\n    Source ({{ ctx.sourceType }}): {{ ctx.content }}\n    {% endfor %}\n\n    Chat History:\n    {% for msg in messages %}\n    {{ _.role(msg.role) }} {{ msg.content }}\n    {% endfor %}\n  \"#\n}\n// New function for generating a document title\nfunction GenerateDocumentTitle(text_chunks: string) -> string {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a helpful AI assistant tasked with generating a concise title for a document.\n    The title should be 5-10 words long, capturing the main theme of the text.\n    Use the following text chunks from the document to determine the title:\n\n    {{ text_chunks }}\n\n    Provide only the title, without any additional explanation.\n  \"#\n}\n\n// New function for generating a subtopic name\nfunction GenerateSubtopicName(subtopic_text: string) -> string {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a helpful AI assistant tasked with generating a concise, descriptive name for a subtopic.\n    The name should be 3-7 words long, capturing the key theme or subject matter.\n    Use the following text excerpt from the subtopic to determine an appropriate name:\n\n    {{ subtopic_text }}\n\n    Provide only the subtopic name, without any additional explanation or leading/trailing punctuation.\n  \"#\n}\n\n\nfunction CheckSubtopicRelevance(text: string) -> float {\n  client GeminiGroqFallback\n  prompt #\"\n    Evaluate the relevance and substantiality of a potential subtopic text segment.\n    Return a single decimal number between 0.0 and 1.0, where:\n    - 0.0-0.3: Low relevance (disconnected sentences, purely navigational, or boilerplate content)\n    - 0.4-0.7: Medium relevance (somewhat informative but lacking depth)\n    - 0.8-1.0: High relevance (substantial, coherent discussion with meaningful details)\n\n    Evaluation criteria:\n    - Contains meaningful information\n    - Discusses a coherent theme\n    - Is not just boilerplate/formatting/references\n    - Has sufficient detail\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ text }}\n  \"#\n}\nclass QueryIntent {\n  requiresGraphQuery bool @description(\"Whether the query requires accessing a graph database\")\n  intentType \"casual_conversation\" | \"graph_query\" | \"unknown\" @description(\"The type of intent detected in the query\")\n  reasoning string @description(\"Explanation for why this classification was made\")\n}\n\nfunction ClassifyQueryIntent(userQuery: string) -> QueryIntent {\n  client GeminiGroqFallback\n  prompt #\"\n    Determine if the given user query requires accessing a graph database or if it's just casual conversation.\n    Consider queries about relationships between entities, data lookup, or structured information as requiring graph queries.\n    Casual conversation includes greetings, small talk, or general questions not requiring data lookup.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ userQuery }}\n  \"#\n}\n\ntest CasualConversationTest {\n  functions [ClassifyQueryIntent]\n  args {\n    userQuery \"Hello! How are you doing today?\"\n  }\n}\n\ntest GraphQueryTest {\n  functions [ClassifyQueryIntent]\n  args {\n    userQuery \"Show me all employees who report to Sarah Johnson and have been at the company for more than 5 years.\"\n  }\n}\ntest BasicLowRelevanceTest {\n  functions [CheckSubtopicRelevance]\n  args {\n    text \"Click here to navigate to the next page. See also: Table of Contents.\"\n  }\n}\n\ntest HighRelevanceTest {\n  functions [CheckSubtopicRelevance]\n  args {\n    text #\"\n      The photosynthesis process in plants consists of two main stages: light-dependent reactions and light-independent reactions. \n      During the light-dependent reactions, chlorophyll absorbs sunlight and converts it into chemical energy in the form of ATP and NADPH.\n      The light-independent reactions, also known as the Calvin cycle, use this energy to produce glucose from carbon dioxide.\n    \"#\n  }\n}\n// Test with a simple chat message\n// Test with different context sources\ntest ChatWithVectorDBContext {\n  functions [StreamingChat]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"What can you tell me about machine learning?\"\n        timestamp \"2024-03-20T10:00:00Z\"\n      }\n    ]\n    context [\n      {\n        sourceType \"vector_db\"\n        content \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"\n        relevanceScore 0.95\n      }\n    ]\n  }\n}\n\ntest ChatWithMultipleContextSources {\n  functions [StreamingChat]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"How do neural networks work?\"\n        timestamp \"2024-03-20T10:05:00Z\"\n      }\n    ]\n    context [\n      {\n        sourceType \"vector_db\"\n        content \"Neural networks are computing systems inspired by biological neural networks in human brains.\"\n        relevanceScore 0.9\n      },\n      {\n        sourceType \"document\"\n        content \"Deep learning neural networks typically consist of multiple layers of interconnected nodes.\"\n        relevanceScore 0.85\n      }\n    ]\n  }\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> Gemini {\n  provider google-ai\n  options {\n    model \"gemini-1.5-flash\"\n    api_key env.GEMINI_API_KEY\n  }\n}\n\nclient<llm> Groq {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API\n    model \"llama3-70b-8192\"\n  }\n}\nclient<llm> GeminiGroqFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [Gemini, Groq]\n  }\n}\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.83.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "graph_rag.baml": "class GraphSchema {\n  nodes string[] @description(\"List of node types with properties, e.g., 'Topic { id (STRING, PK), name (STRING) }'\")\n  relationships string[] @description(\"List of relationship types with connections and properties, e.g., 'SUBTOPIC_OF (Subtopic -> Topic) { position (UINT32) }'\")\n  properties string[] @description(\"List of property definitions across nodes and relationships, e.g., 'id (STRING, PK)'\")\n}\n\nclass GraphQuery {\n  query string @description(\"The OpenCypher query to execute\")\n}\n\nclass GraphResult {\n  result string @description(\"The raw result from the graph database query\")\n}\n\nclass FinalResponse {\n  answer string @stream.with_state @description(\"Natural language answer based on the query results\")\n  queryUsed string @stream.done @description(\"The Cypher query that was used\")\n  rawResults string @stream.done @description(\"The raw results that were returned\")\n}\nfunction GenerateJsonQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\nfunction GenerateGraphQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\nfunction GenerateGraphQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a graph database expert. Generate an OpenCypher query based on the user's question and the available schema.\n    Keep the query simple, focused on answering the specific question, and valid for KuzuDB.\n    ### Few-Shot Examples:\n    1. **Question**: \"What is the name of the person with ID 'p1'?\"\n       **Query**: \"MATCH (p:JSON_Person {id: 'p1'}) RETURN p.name\"\n\n    2. **Question**: \"List all people older than 30.\"\n       **Query**: \"MATCH (p:JSON_Person) WHERE p.age > 30 RETURN p.name, p.age\"\n\n    3. **Question**: \"Which people have a health insurance provider named 'Blue Cross Blue Shield'?\"\n       **Query**: \"MATCH (p:JSON_Person) WHERE ANY(ip IN p.info.insurance_provider WHERE ip.name = 'Blue Cross Blue Shield') RETURN p.name\"\n\n    4. **Question**: \"How many people have a height greater than 1.7 meters?\"\n       **Query**: \"MATCH (p:JSON_Person) WHERE p.info.height > 1.7 RETURN COUNT(p) AS tall_people\"\n\n    5. **Question**: \"What are the names and policy numbers of people with health insurance?\"\n       **Query**: \"MATCH (p:JSON_Person) WHERE ANY(ip IN p.info.insurance_provider WHERE ip.type = 'health') RETURN p.name, [ip IN p.info.insurance_provider WHERE ip.type = 'health' | ip.policy_number] AS policy_numbers\"\n\n    6. **Question**: \"Which people are connected to the condition 'Diabetes'?\"\n       **Query**: \"MATCH (p:JSON_Person)-[:HAS_CONDITION]->(c:JSON_Condition {name: 'Diabetes'}) RETURN p.name, c.name\"\n\n    7. **Question**: \"List all conditions with the number of people affected, sorted by count.\"\n       **Query**: \"MATCH (p:JSON_Person)-[:HAS_CONDITION]->(c:JSON_Condition) RETURN c.name, COUNT(p) AS affected_count ORDER BY affected_count DESC\"\n\n    8. **Question**: \"What people have names containing 'Greg'?\"\n       **Query**: \"MATCH (p:JSON_Person) WHERE p.name CONTAINS 'Greg' RETURN p.name, p.id\"\n\n    9. **Question**: \"Who has been affected by a condition since before 2018?\"\n       **Query**: \"MATCH (p:JSON_Person)-[r:HAS_CONDITION]->(c:JSON_Condition) WHERE r.since < 2018 RETURN p.name, c.name, r.since\"\n\n    10. **Question**: \"What are all the properties of the person with ID 'p2'?\"\n        **Query**: \"MATCH (p:JSON_Person {id: 'p2'}) RETURN p\"\n    Available Schema:\n    Nodes: {{ schema.nodes | join(', ') }}\n    Relationships: {{ schema.relationships | join(', ') }}\n    Properties: {{ schema.properties | join(', ') }}\n    ### Instructions:\n    - Use exact node labels (e.g., 'Topic', 'Subtopic') and relationship types (e.g., 'SUBTOPIC_OF') from the schema.\n    - Match properties exactly as listed (e.g., 'id', 'name', 'text').\n    - For text searches, use 'CONTAINS' for partial matches.\n    - Return only the relevant data to answer the question.\n    - Avoid invalid functions like 'TYPE' (use 'type(r)' only if explicitly needed and supported).\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ question }}\n  \"#\n}\nfunction GenerateGraphQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\nfunction GenerateGraphQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\nfunction GenerateGraphQuery(schema: GraphSchema, question: string) -> GraphQuery {\n  client GeminiGroqFallback\n  prompt #\"\n    You are a graph database expert. Generate an OpenCypher query based on the user's question and the available schema.\n    Keep the query simple, focused on answering the specific question, and valid for KuzuDB.\n\n    Available Schema:\n    Nodes: {{ schema.nodes | join(', ') }}\n    Relationships: {{ schema.relationships | join(', ') }}\n    Properties: {{ schema.properties | join(', ') }}\n\n    ### Few-Shot Examples:\n    1. **Question**: \"What is the name of the topic with ID 'ff0e7064-dcc9-4b00-a712-28b703574ba4'?\"\n       **Query**: \"MATCH (t:Topic {id: 'ff0e7064-dcc9-4b00-a712-28b703574ba4'}) RETURN t.name\"\n\n    2. **Question**: \"What subtopics are under the topic 'Barcode Scanning Procedure: Align and Capture Barcode Data'?\"\n       **Query**: \"MATCH (t:Topic {name: 'Barcode Scanning Procedure: Align and Capture Barcode Data'})<-[:SUBTOPIC_OF]-(s:Subtopic) RETURN s.name, s.text\"\n\n    3. **Question**: \"Which topics have subtopics mentioning 'barcode'?\"\n       **Query**: \"MATCH (t:Topic)<-[:SUBTOPIC_OF]-(s:Subtopic) WHERE s.text CONTAINS 'barcode' RETURN t.name\"\n\n    4. **Question**: \"List all topics with their subtopic counts.\"\n       **Query**: \"MATCH (t:Topic)<-[:SUBTOPIC_OF]-(s:Subtopic) RETURN t.name, COUNT(s) AS subtopic_count\"\n\n    5. **Question**: \"What are the bullet points for subtopics under 'Barcode Scanning Procedure: Align and Capture Barcode Data'?\"\n       **Query**: \"MATCH (t:Topic {name: 'Barcode Scanning Procedure: Align and Capture Barcode Data'})<-[:SUBTOPIC_OF]-(s:Subtopic) RETURN s.bullet_points\"\n\n    ### Instructions:\n    - Use exact node labels (e.g., 'Topic', 'Subtopic') and relationship types (e.g., 'SUBTOPIC_OF') from the schema.\n    - Match properties exactly as listed (e.g., 'id', 'name', 'text').\n    - For text searches, use 'CONTAINS' for partial matches.\n    - Return only the relevant data to answer the question.\n    - Avoid invalid functions like 'TYPE' (use 'type(r)' only if explicitly needed and supported).\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} {{ question }}\n  \"#\n}\n\nfunction AnalyzeResults(question: string, query: string, results: GraphResult) -> FinalResponse {\n  client GeminiGroqFallback\n  prompt #\"\n    Analyze the results from the graph database query and provide a natural language response.\n    Explain the findings clearly and conversationally.\n\n    Original Question: {{ question }}\n    Query Used: {{ query }}\n    Query Results: {{ results.result }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest TestGenerateGraphQuery {\n  functions [GenerateGraphQuery]\n  args {\n    schema {\n      nodes [\"Person\", \"Movie\"]\n      relationships [\"ACTED_IN\", \"DIRECTED\"]\n      properties [\"name\", \"title\", \"year\"]\n    }\n    question \"Who directed movies in 2020?\"\n  }\n}\n\ntest TestAnalyzeResults {\n  functions [AnalyzeResults]\n  args {\n    question \"Who directed movies in 2020?\"\n    query \"MATCH (p:Person)-[:DIRECTED]->(m:Movie) WHERE m.year = 2020 RETURN p.name, m.title\"\n    results {\n      result \"[{'p.name': 'Christopher Nolan', 'm.title': 'Tenet'}]\"\n    }\n  }\n}",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map